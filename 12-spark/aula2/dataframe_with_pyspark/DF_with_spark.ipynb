{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6278d55",
   "metadata": {},
   "source": [
    "# DataFrame com Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f13359-4d5b-49f3-b32b-d4d9777ae67b",
   "metadata": {},
   "source": [
    "## 1. Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3823300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando os pacotes necessários\n",
    "\n",
    "#!conda install pyspark -y\n",
    "#!conda install -c conda-forge findspark -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8315a27-b949-42aa-af61-b496302f487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9730d37-0e5d-4f48-87db-aa4468a8e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/16 08:45:38 WARN Utils: Your hostname, hub-G3-3590 resolves to a loopback address: 127.0.1.1; using 192.168.100.249 instead (on interface enp3s0)\n",
      "22/02/16 08:45:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/16 08:45:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Iniciando uma secção spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87058ffa",
   "metadata": {},
   "source": [
    "## 2. Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e84d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------+--------+\n",
      "|ClienteID|             Cliente|Estado|Genero|  Status|\n",
      "+---------+--------------------+------+------+--------+\n",
      "|        1|Adelina Buenaventura|    RJ|     M|  Silver|\n",
      "|        2|        Adelino Gago|    RJ|     M|  Silver|\n",
      "|        3|     Adolfo Patrício|    PE|     M|  Silver|\n",
      "|        4|    Adriana Guedelha|    RO|     F|Platinum|\n",
      "|        5|       Adélio Lisboa|    SE|     M|  Silver|\n",
      "|        6|       Adérito Bahía|    MA|     M|  Silver|\n",
      "|        7|       Aida Dorneles|    RN|     F|  Silver|\n",
      "|        8|   Alarico Quinterno|    AC|     M|  Silver|\n",
      "|        9|    Alberto Cezimbra|    AM|     M|  Silver|\n",
      "|       10|    Alberto Monsanto|    RN|     M|    Gold|\n",
      "|       11|       Albino Canela|    AC|     M|  Silver|\n",
      "|       12|     Alceste Varanda|    RR|     F|  Silver|\n",
      "|       13|  Alcides Carvalhais|    RO|     M|  Silver|\n",
      "|       14|        Aldo Martins|    GO|     M|  Silver|\n",
      "|       15|   Alexandra Tabares|    MG|     F|  Silver|\n",
      "|       16|      Alfredo Cotrim|    SC|     M|  Silver|\n",
      "|       17|     Almeno Figueira|    SC|     M|  Silver|\n",
      "|       18|      Alvito Peralta|    AM|     M|  Silver|\n",
      "|       19|     Amadeu Martinho|    RN|     M|  Silver|\n",
      "|       20|      Amélia Estévez|    PE|     F|  Silver|\n",
      "+---------+--------------------+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clientes = spark.read.parquet('dataset/Clientes.parquet', inferSchema=True)\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84cc5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ClienteID: long (nullable = true)\n",
      " |-- Cliente: string (nullable = true)\n",
      " |-- Estado: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e84d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+---------+--------+\n",
      "|VendasID|VendedorID|ClienteID|     Data|   Total|\n",
      "+--------+----------+---------+---------+--------+\n",
      "|       1|         1|       91| 1/1/2019|  8053.6|\n",
      "|       2|         6|      185| 1/1/2020|   150.4|\n",
      "|       3|         7|       31| 2/1/2020|  6087.0|\n",
      "|       4|         5|       31| 2/1/2019| 13828.6|\n",
      "|       5|         5|       31| 3/1/2018|26096.66|\n",
      "|       6|         5|       31| 4/1/2020| 18402.0|\n",
      "|       7|         5|       31| 6/1/2019|  7524.2|\n",
      "|       8|         5|      186| 6/1/2019| 12036.6|\n",
      "|       9|         7|       91| 6/1/2020| 2804.75|\n",
      "|      10|         2|      202| 6/1/2020|  8852.0|\n",
      "|      11|         7|       58| 8/1/2019|16545.25|\n",
      "|      12|         7|       58| 9/1/2018|11411.88|\n",
      "|      13|         7|       58|10/1/2019| 15829.7|\n",
      "|      14|         3|      249|12/1/2020| 6154.36|\n",
      "|      15|         4|      249|12/1/2018| 3255.08|\n",
      "|      16|         7|      192|13/1/2020| 2901.25|\n",
      "|      17|         2|       79|13/1/2019| 15829.7|\n",
      "|      18|        10|       79|14/1/2019|16996.36|\n",
      "|      19|        10|      191|14/1/2019|   155.0|\n",
      "|      20|         9|      218|15/1/2018|  131.75|\n",
      "+--------+----------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendas = spark.read.parquet('dataset/Vendas.parquet', inerSchema=True)\n",
    "vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41b9b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendasID: long (nullable = true)\n",
      " |-- VendedorID: long (nullable = true)\n",
      " |-- ClienteID: long (nullable = true)\n",
      " |-- Data: string (nullable = true)\n",
      " |-- Total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ca5f9",
   "metadata": {},
   "source": [
    "## 3. Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec41d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|             Cliente|Estado|  Status|\n",
      "+--------------------+------+--------+\n",
      "|Adelina Buenaventura|    RJ|  Silver|\n",
      "|        Adelino Gago|    RJ|  Silver|\n",
      "|     Adolfo Patrício|    PE|  Silver|\n",
      "|    Adriana Guedelha|    RO|Platinum|\n",
      "|       Adélio Lisboa|    SE|  Silver|\n",
      "|       Adérito Bahía|    MA|  Silver|\n",
      "|       Aida Dorneles|    RN|  Silver|\n",
      "|   Alarico Quinterno|    AC|  Silver|\n",
      "|    Alberto Cezimbra|    AM|  Silver|\n",
      "|    Alberto Monsanto|    RN|    Gold|\n",
      "|       Albino Canela|    AC|  Silver|\n",
      "|     Alceste Varanda|    RR|  Silver|\n",
      "|  Alcides Carvalhais|    RO|  Silver|\n",
      "|        Aldo Martins|    GO|  Silver|\n",
      "|   Alexandra Tabares|    MG|  Silver|\n",
      "|      Alfredo Cotrim|    SC|  Silver|\n",
      "|     Almeno Figueira|    SC|  Silver|\n",
      "|      Alvito Peralta|    AM|  Silver|\n",
      "|     Amadeu Martinho|    RN|  Silver|\n",
      "|      Amélia Estévez|    PE|  Silver|\n",
      "+--------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reordenado = clientes.select('Cliente', 'Estado', 'Status')\n",
    "df_reordenado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121aadc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+------+--------+\n",
      "|ClienteID|            Cliente|Estado|Genero|  Status|\n",
      "+---------+-------------------+------+------+--------+\n",
      "|        4|   Adriana Guedelha|    RO|     F|Platinum|\n",
      "|       10|   Alberto Monsanto|    RN|     M|    Gold|\n",
      "|       28|      Anna Carvajal|    RS|     F|    Gold|\n",
      "|       49|      Bento Quintão|    SP|     M|    Gold|\n",
      "|       68|      Carminda Dias|    AM|     F|    Gold|\n",
      "|       83|      Cláudio Jorge|    TO|     M|    Gold|\n",
      "|      121|    Dionísio Saltão|    PR|     M|    Gold|\n",
      "|      166|   Firmino Meireles|    AM|     M|    Gold|\n",
      "|      170|      Flor Vilanova|    CE|     M|Platinum|\n",
      "|      220|Honorina Villaverde|    PE|     F|    Gold|\n",
      "|      230|    Ibijara Botelho|    RR|     F|Platinum|\n",
      "|      237|  Iracema Rodríguez|    BA|     F|    Gold|\n",
      "|      247|         Joana Ataí|    GO|     F|Platinum|\n",
      "+---------+-------------------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = clientes.where('Status == \"Platinum\" or Status == \"Gold\"')\n",
    "df_filtrado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d5386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a650fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|  Status|count(Status)|\n",
      "+--------+-------------+\n",
      "|  Silver|          237|\n",
      "|    Gold|            9|\n",
      "|Platinum|            4|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cont_status = clientes.groupby('Status').agg(count('Status')).orderBy('count(status)', ascending=False)\n",
    "cont_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a650fa6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg() got an unexpected keyword argument 'contagem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7801/3217781770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcont_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontagem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count(status)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcont_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: agg() got an unexpected keyword argument 'contagem'"
     ]
    }
   ],
   "source": [
    "cont_status = clientes.groupby('Status').agg(contagem=('Status', 'count')).orderBy('count(status)', ascending=False)\n",
    "cont_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee55cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.select(\"firstName\", \"age\") \\\n",
    "#.write \\\n",
    "#.save(\"namesAndAges.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60eec7",
   "metadata": {},
   "source": [
    "## 4. Salvando os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "127db9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reordenado.write.mode('overwrite').format('csv').save('dataset/resultados/df_reordenado_csv')\n",
    "df_reordenado.write.mode('overwrite').format('json').save('dataset/resultados/df_reordenado_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "127db9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_filtrado.write.mode('overwrite').format('orc').save('dataset/resultados/df_filtrado_orc')\n",
    "df_filtrado.write.mode('overwrite').format('parquet').save('dataset/resultados/df_filtrado_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e00dc8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Status', 'Qtd_status']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O spark não salva o DataFrame por que não aceita os caracteres do nome da\n",
    "# coluna que foi agregada\n",
    "cont_status = cont_status.withColumnRenamed('count(Status)', 'Qtd_status')\n",
    "cont_status.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "127db9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_status.write.mode('overwrite').format('csv').save('dataset/resultados/cont_status_csv')\n",
    "cont_status.write.mode('overwrite').format('json').save('dataset/resultados/cont_status_json')\n",
    "cont_status.write.mode('overwrite').format('orc').save('dataset/resultados/cont_status_orc')\n",
    "cont_status.write.mode('overwrite').format('parquet').save('dataset/resultados/cont_status_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8eea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
